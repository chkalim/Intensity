{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HQ-TWHAnhIe",
        "outputId": "60d91d77-f579-4a31-8e27-ed6e789ef099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4a9jYmIv6Qhr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as Data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "import random\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzXyQ_ii6Qhx"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8wEqM_y46Qh0"
      },
      "outputs": [],
      "source": [
        "#  predition TI of leading time at 24 hours\n",
        "pre_seq = 4\n",
        "batch_size = 128\n",
        "epochs = 128\n",
        "min_val_loss = 100\n",
        "model_name = 'SAF_Net.pkl'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wGMIKq336Qh1"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/intensity/TI_Prediction-master/data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
        "test= pd.read_csv('/content/drive/My Drive/Colab Notebooks/intensity/TI_Prediction-master/data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3mRf70Q6Qh4",
        "outputId": "01692411-a823-483c-dd15-b8252ee3776a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8406, 101), (2747, 101))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train.shape, test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "crasYUyh6Qh5"
      },
      "outputs": [],
      "source": [
        "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
        "CLIPER_feature.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t5Sja7ar6Qh7"
      },
      "outputs": [],
      "source": [
        "X_wide_scaler = MinMaxScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "\n",
        "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 5:])\n",
        "X_wide_train = X_wide[0: train.shape[0], :]\n",
        "\n",
        "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3].values.reshape(-1, 1))\n",
        "y_train = y[0: train.shape[0], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q-2NuhPY6Qh8"
      },
      "outputs": [],
      "source": [
        "\n",
        "ahead_times = [0]\n",
        "pressures = [1, 2, 3,5, 7, 10,20, 30, 50,70, 100, 125,150, 175, 200,225,\n",
        "             250, 300,350, 400, 450,500, 550, 600,650, 700, 750,775,\n",
        "             800, 825,850, 875, 900,925, 950, 975,1000]\n",
        "\n",
        "\n",
        "\n",
        "sequential_reanalysis_u_list = []\n",
        "reanalysis_u_test_dict = {}\n",
        "X_deep_u_scaler_dict = {}\n",
        "\n",
        "sequential_reanalysis_v_list = []\n",
        "reanalysis_v_test_dict = {}\n",
        "X_deep_v_scaler_dict = {}\n",
        "\n",
        "sequential_reanalysis_t_list = []\n",
        "reanalysis_t_test_dict = {}\n",
        "X_deep_t_scaler_dict = {}\n",
        "\n",
        "\n",
        "sequential_reanalysis_r_list = []\n",
        "reanalysis_r_test_dict = {}\n",
        "X_deep_r_scaler_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHW1Vzeivgv1",
        "outputId": "a59bb85a-e3f2-4ac4-bc4f-b2d2a7a28a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahead_time: 0 (8406, 1, 37, 3, 3, 1)\n"
          ]
        }
      ],
      "source": [
        "reanalysis_type = 'v'\n",
        "for ahead_time in ahead_times:\n",
        "    reanalysis_list = []\n",
        "    for pressure in pressures:\n",
        "        folder = None\n",
        "        if ahead_time == 0:\n",
        "            folder = reanalysis_type\n",
        "        else:\n",
        "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
        "        all_reanalysis_csv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/ERA_Interim/'+folder+'/reanalysis_'+reanalysis_type+'_'+str(pressure)+'.csv', header=None)\n",
        "      \n",
        "\n",
        "        train_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(train[0].unique())]\n",
        "        test_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(test[0].unique())]\n",
        "        reanalysis_v_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
        "\n",
        "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
        "        reanalysis.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
        "        X_deep_v_scaler_dict[scaler_name] = MinMaxScaler()\n",
        "        \n",
        "        # 5:end is the 31*31 v component wind speed\n",
        "        X_deep = X_deep_v_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
        "        \n",
        "        # (batch, type, channel, height, widht, time) here type is v\n",
        "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 3, 3, 1)\n",
        "        reanalysis_list.append(X_deep_final)\n",
        "        \n",
        "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
        "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
        "    sequential_reanalysis_v_list.append(X_deep_temp)\n",
        "\n",
        "X_deep_v_train = np.concatenate(sequential_reanalysis_v_list, axis=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVu8oCxx6Qh-",
        "outputId": "dbfc91d9-d5ee-4d0d-9ea0-ecd28bb6fab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahead_time: 0 (8406, 1, 37, 3, 3, 1)\n"
          ]
        }
      ],
      "source": [
        "reanalysis_type = 't'\n",
        "for ahead_time in ahead_times:\n",
        "    reanalysis_list = []\n",
        "    for pressure in pressures:\n",
        "        folder = None\n",
        "        if ahead_time == 0:\n",
        "            folder = reanalysis_type\n",
        "        else:\n",
        "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
        "\n",
        "        all_reanalysis_csv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/ERA_Interim/'+folder+'/reanalysis_'+reanalysis_type+'_'+str(pressure)+'.csv', header=None)\n",
        "      \n",
        "        train_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(train[0].unique())]\n",
        "        test_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(test[0].unique())]\n",
        "        reanalysis_t_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
        "        \n",
        "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
        "        reanalysis.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
        "        X_deep_t_scaler_dict[scaler_name] = MinMaxScaler()\n",
        "        \n",
        "        # 5:end is the 31*31 u component wind speed\n",
        "        X_deep = X_deep_t_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
        "        \n",
        "        # (batch, type, channel, height, widht, time) here type is u\n",
        "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 3, 3, 1)\n",
        "        reanalysis_list.append(X_deep_final)\n",
        "\n",
        "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
        "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
        "    sequential_reanalysis_t_list.append(X_deep_temp)\n",
        "\n",
        "X_deep_t_train = np.concatenate(sequential_reanalysis_t_list, axis=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMX-BMc7W-oe",
        "outputId": "6e45a24b-d6d7-49ec-8975-18cc0eab1fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahead_time: 0 (8406, 1, 37, 3, 3, 1)\n"
          ]
        }
      ],
      "source": [
        "reanalysis_type = 'u'\n",
        "for ahead_time in ahead_times:\n",
        "    reanalysis_list = []\n",
        "    for pressure in pressures:\n",
        "        folder = None\n",
        "        if ahead_time == 0:\n",
        "            folder = reanalysis_type\n",
        "        else:\n",
        "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
        "\n",
        "        all_reanalysis_csv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/ERA_Interim/'+folder+'/reanalysis_'+reanalysis_type+'_'+str(pressure)+'.csv', header=None)\n",
        "      \n",
        "\n",
        "        train_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(train[0].unique())]\n",
        "        test_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(test[0].unique())]\n",
        "        reanalysis_u_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
        "        \n",
        "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
        "        reanalysis.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
        "        X_deep_u_scaler_dict[scaler_name] = MinMaxScaler()\n",
        "        \n",
        "        # 5:end is the 31*31 u component wind speed\n",
        "        X_deep = X_deep_u_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
        "        \n",
        "        # (batch, type, channel, height, widht, time) here type is u\n",
        "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 3, 3, 1)\n",
        "        reanalysis_list.append(X_deep_final)\n",
        "\n",
        "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
        "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
        "    sequential_reanalysis_u_list.append(X_deep_temp)\n",
        "\n",
        "X_deep_u_train = np.concatenate(sequential_reanalysis_u_list, axis=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWn467wDXCWS",
        "outputId": "022b3ce0-c099-461b-f34c-3db97372ba8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahead_time: 0 (8406, 1, 37, 3, 3, 1)\n"
          ]
        }
      ],
      "source": [
        "reanalysis_type = 'r'\n",
        "for ahead_time in ahead_times:\n",
        "    reanalysis_list = []\n",
        "    for pressure in pressures:\n",
        "        folder = None\n",
        "        if ahead_time == 0:\n",
        "            folder = reanalysis_type\n",
        "        else:\n",
        "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
        "\n",
        "        all_reanalysis_csv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/ERA_Interim/'+folder+'/reanalysis_'+reanalysis_type+'_'+str(pressure)+'.csv', header=None)\n",
        "      \n",
        "\n",
        "        train_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(train[0].unique())]\n",
        "        test_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(test[0].unique())]\n",
        "        reanalysis_r_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
        "        \n",
        "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
        "        reanalysis.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
        "        X_deep_r_scaler_dict[scaler_name] = MinMaxScaler()\n",
        "        \n",
        "        # 5:end is the 31*31 u component wind speed\n",
        "        X_deep = X_deep_r_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
        "        \n",
        "        # (batch, type, channel, height, widht, time) here type is u\n",
        "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 3, 3, 1)\n",
        "        reanalysis_list.append(X_deep_final)\n",
        "\n",
        "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
        "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
        "    sequential_reanalysis_r_list.append(X_deep_temp)\n",
        "\n",
        "X_deep_r_train = np.concatenate(sequential_reanalysis_r_list, axis=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "01K0MPdV6QiC"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_deep_train = np.concatenate((X_deep_u_train, X_deep_v_train,X_deep_t_train,X_deep_r_train), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TKeiori6QiD",
        "scrolled": true,
        "outputId": "cc040d6b-ecfd-48f7-fe8f-4580e426a969"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8406, 96), (8406, 4, 37, 3, 3, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X_wide_train.shape, X_deep_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3LytQxr6QiE"
      },
      "source": [
        "# training set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y32yiRNg6QiE"
      },
      "outputs": [],
      "source": [
        "class TrainLoader(Data.Dataset):\n",
        "    def __init__(self, X_wide_train, X_deep_train, y_train):\n",
        "        self.X_wide_train = X_wide_train\n",
        "        self.X_deep_train = X_deep_train\n",
        "        self.y_train = y_train\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return [self.X_wide_train[index], self.X_deep_train[index]], self.y_train[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X_wide_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sPODW5Nf6QiG"
      },
      "outputs": [],
      "source": [
        "full_train_index = [*range(0, len(X_wide_train))]\n",
        "\n",
        "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPCfXwOQ6QiG",
        "scrolled": true,
        "outputId": "b29b7378-fd35-4c41-c1f3-298715e357e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7565, 841)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(train_index), len(val_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GX4zv_UZ6QiG"
      },
      "outputs": [],
      "source": [
        "train_dataset = torch.utils.data.DataLoader(\n",
        "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
        "                                                 batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TpD4FI4x6QiI"
      },
      "outputs": [],
      "source": [
        "val_dataset = torch.utils.data.DataLoader(\n",
        "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
        "                                                 batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRTsIwMu6QiJ"
      },
      "source": [
        "# SAF-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "u9xd8v1Z1r8U"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # deep\n",
        "        self.conv1 = nn.Conv2d(37, 64, kernel_size=1, padding=(1, 1))\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=1, padding=(1, 1))\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=1, padding=(1, 1))\n",
        "        self.fc1 = nn.Linear(256 *2 *2, 128)\n",
        "\n",
        "        self.fc2 = nn.Linear(96 + 128 * len(ahead_times), 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.fc4=nn.Linear(128,1)\n",
        "\n",
        "        self.num_layers=1\n",
        "        self.hidden_size=128\n",
        "        self.gru = nn.GRU(input_size=96, hidden_size=128,\n",
        "                            num_layers=1, batch_first=True)\n",
        "        self.gru2 = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers, batch_first=True)\n",
        "        self.gru3 = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers, batch_first=True)\n",
        "        self.gru4 = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, wide, deep):\n",
        "        seq_list = []\n",
        "        seq_list2 = []\n",
        "        seq_list3 = []\n",
        "        seq_list4 = []\n",
        "\n",
        "        x = wide.unsqueeze(1)\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size,device=x.device).float())\n",
        "        h_1 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size,device=x.device).float())\n",
        "        h_2 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size,device=x.device).float())\n",
        "        h_3 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size,device=x.device).float())\n",
        "        ula,_  = self.gru(x, h_0)\n",
        "        ula2, _ = self.gru2(ula,h_1)\n",
        "        ula3, _ = self.gru3(ula2, h_2)\n",
        "        ula4, _ = self.gru4(ula3, h_3)\n",
        "        h_out = ula4.view(-1, self.hidden_size)\n",
        "        h_out = self.fc4(h_out)\n",
        "        for i in range(len(ahead_times)):\n",
        "            \n",
        "            deep_u = deep[:,0,:,:,:,i]\n",
        "            deep_v = deep[:,1,:,:,:,i]\n",
        "            deep_t = deep[:,2,:,:,:,i]\n",
        "            deep_r = deep[:,3,:,:,:,i]\n",
        "            \n",
        "           \n",
        "            deep_u = self.pool(F.relu(self.conv1(deep_u)))\n",
        "            deep_v = self.pool(F.relu(self.conv1(deep_v)))\n",
        "            deep_t = self.pool(F.relu(self.conv1(deep_t)))\n",
        "            deep_r = self.pool(F.relu(self.conv1(deep_r)))\n",
        "           \n",
        "           \n",
        "            deep_u = self.pool(F.relu(self.conv2(deep_u))) \n",
        "            deep_v = self.pool(F.relu(self.conv2(deep_v)))\n",
        "            deep_t = self.pool(F.relu(self.conv2(deep_t))) \n",
        "            deep_r = self.pool(F.relu(self.conv2(deep_r)))\n",
        "            \n",
        "\n",
        "           \n",
        "            deep_u = self.pool(F.relu(self.conv3(deep_u)))\n",
        "            deep_v = self.pool(F.relu(self.conv3(deep_v)))\n",
        "            deep_t = self.pool(F.relu(self.conv3(deep_t)))\n",
        "            deep_r = self.pool(F.relu(self.conv3(deep_r)))\n",
        "\n",
        "\n",
        "           \n",
        "            time_seq = deep_u.view(-1, 256 * 2 * 2)\n",
        "            time_seq2 = deep_v.view(-1, 256 * 2 * 2)\n",
        "            time_seq3 = deep_t.view(-1, 256 * 2 * 2)\n",
        "            time_seq4 = deep_r.view(-1, 256 * 2 * 2)\n",
        "\n",
        "            time_seq = self.fc1(time_seq)\n",
        "            time_seq2 = self.fc1(time_seq2)\n",
        "            time_seq3 = self.fc1(time_seq3)\n",
        "            time_seq4 = self.fc1(time_seq4)\n",
        "\n",
        "\n",
        "            seq_list.append(time_seq)\n",
        "            seq_list2.append(time_seq2)\n",
        "            seq_list3.append(time_seq3)\n",
        "            seq_list4.append(time_seq4)\n",
        "\n",
        "\n",
        "        wide = wide.view(-1, 96)\n",
        "        wide_n_deep_u = torch.cat((wide, seq_list[0]),1)\n",
        "        if len(ahead_times) > 1:\n",
        "            for i in range(1, len(ahead_times)):\n",
        "                wide_n_deep = torch.cat((wide_n_deep_u, seq_list[i]),1)\n",
        "        wide_n_deep_u = F.relu(self.fc2(wide_n_deep_u))\n",
        "        wide_n_deep_u = F.relu(self.fc3(wide_n_deep_u))\n",
        "\n",
        "        wide_n_deep_v = torch.cat((wide, seq_list2[0]),1)\n",
        "        if len(ahead_times) > 1:\n",
        "            for i in range(1, len(ahead_times)):\n",
        "                wide_n_deep_v = torch.cat((wide_n_deep_v, seq_list2[i]),1)\n",
        "        wide_n_deep_v = F.relu(self.fc2(wide_n_deep_v))\n",
        "        wide_n_deep_v = F.relu(self.fc3(wide_n_deep_v))\n",
        "\n",
        "\n",
        "        wide_n_deep_t = torch.cat((wide, seq_list3[0]),1)\n",
        "        if len(ahead_times) > 1:\n",
        "            for i in range(1, len(ahead_times)):\n",
        "                wide_n_deep_t = torch.cat((wide_n_deep_t, seq_list3[i]),1)\n",
        "        wide_n_deep_t = F.relu(self.fc2(wide_n_deep_t))\n",
        "        wide_n_deep_t = F.relu(self.fc3(wide_n_deep_t))\n",
        "\n",
        "        wide_n_deep_r = torch.cat((wide, seq_list4[0]),1)\n",
        "        if len(ahead_times) > 1:\n",
        "            for i in range(1, len(ahead_times)):\n",
        "                wide_n_deep_r = torch.cat((wide_n_deep_r, seq_list4[i]),1)\n",
        "        wide_n_deep_r = F.relu(self.fc2(wide_n_deep_r))\n",
        "        wide_n_deep_r = F.relu(self.fc3(wide_n_deep_r))\n",
        "\n",
        "        return (wide_n_deep_u+wide_n_deep_v+wide_n_deep_t+wide_n_deep_r+h_out)/5\n",
        "    \n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xGjSttah6QiM"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rovVUEpc6QiN"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "asdakmwC6QiN"
      },
      "outputs": [],
      "source": [
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.0007)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot5O0ZX151-M",
        "outputId": "f520673d-327c-4cac-8f3f-5932fcf46f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(device=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-31895G6QiO",
        "scrolled": true,
        "outputId": "28606cb5-fcdc-407a-f5b6-9d9ed1cfc76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs [1/128] train_loss: 7.99468 val_loss: 0.60397\n",
            "epochs [2/128] train_loss: 4.54784 val_loss: 0.48214\n",
            "epochs [3/128] train_loss: 4.16145 val_loss: 0.51370\n",
            "epochs [4/128] train_loss: 4.15420 val_loss: 0.49393\n",
            "epochs [5/128] train_loss: 3.96354 val_loss: 0.47212\n",
            "epochs [6/128] train_loss: 3.88372 val_loss: 0.51077\n",
            "epochs [7/128] train_loss: 3.89883 val_loss: 0.46134\n",
            "epochs [8/128] train_loss: 3.72554 val_loss: 0.45048\n",
            "epochs [9/128] train_loss: 3.69347 val_loss: 0.41587\n",
            "epochs [10/128] train_loss: 3.66802 val_loss: 0.44055\n",
            "epochs [11/128] train_loss: 3.59499 val_loss: 0.44259\n",
            "epochs [12/128] train_loss: 3.58375 val_loss: 0.46458\n",
            "epochs [13/128] train_loss: 3.57938 val_loss: 0.43170\n",
            "epochs [14/128] train_loss: 3.52385 val_loss: 0.40284\n",
            "epochs [15/128] train_loss: 3.56099 val_loss: 0.44123\n",
            "epochs [16/128] train_loss: 3.56128 val_loss: 0.42358\n",
            "epochs [17/128] train_loss: 3.44547 val_loss: 0.42707\n",
            "epochs [18/128] train_loss: 3.49681 val_loss: 0.38044\n",
            "epochs [19/128] train_loss: 3.42898 val_loss: 0.40376\n",
            "epochs [20/128] train_loss: 3.43514 val_loss: 0.37599\n",
            "epochs [21/128] train_loss: 3.38363 val_loss: 0.40946\n",
            "epochs [22/128] train_loss: 3.32134 val_loss: 0.38812\n",
            "epochs [23/128] train_loss: 3.30916 val_loss: 0.37986\n",
            "epochs [24/128] train_loss: 3.25998 val_loss: 0.38831\n",
            "epochs [25/128] train_loss: 3.25769 val_loss: 0.41625\n",
            "epochs [26/128] train_loss: 3.26642 val_loss: 0.37944\n",
            "epochs [27/128] train_loss: 3.26928 val_loss: 0.37897\n",
            "epochs [28/128] train_loss: 3.21197 val_loss: 0.34760\n",
            "epochs [29/128] train_loss: 3.24917 val_loss: 0.44416\n",
            "epochs [30/128] train_loss: 3.21049 val_loss: 0.36697\n",
            "epochs [31/128] train_loss: 3.22664 val_loss: 0.36642\n",
            "epochs [32/128] train_loss: 3.10655 val_loss: 0.38851\n",
            "epochs [33/128] train_loss: 3.12343 val_loss: 0.35959\n",
            "epochs [34/128] train_loss: 3.09672 val_loss: 0.34236\n",
            "epochs [35/128] train_loss: 3.07307 val_loss: 0.37069\n",
            "epochs [36/128] train_loss: 3.01726 val_loss: 0.34374\n",
            "epochs [37/128] train_loss: 3.07922 val_loss: 0.36771\n",
            "epochs [38/128] train_loss: 3.04641 val_loss: 0.34589\n",
            "epochs [39/128] train_loss: 3.03586 val_loss: 0.35635\n",
            "epochs [40/128] train_loss: 2.98353 val_loss: 0.35854\n",
            "epochs [41/128] train_loss: 2.96991 val_loss: 0.34673\n",
            "epochs [42/128] train_loss: 2.94214 val_loss: 0.34015\n",
            "epochs [43/128] train_loss: 2.97860 val_loss: 0.34806\n",
            "epochs [44/128] train_loss: 2.97628 val_loss: 0.33696\n",
            "epochs [45/128] train_loss: 2.92638 val_loss: 0.35106\n",
            "epochs [46/128] train_loss: 3.10273 val_loss: 0.31587\n",
            "epochs [47/128] train_loss: 2.96402 val_loss: 0.35508\n",
            "epochs [48/128] train_loss: 2.96406 val_loss: 0.37431\n",
            "epochs [49/128] train_loss: 2.96845 val_loss: 0.36182\n",
            "epochs [50/128] train_loss: 2.94313 val_loss: 0.32791\n",
            "epochs [51/128] train_loss: 2.91661 val_loss: 0.33275\n",
            "epochs [52/128] train_loss: 2.90891 val_loss: 0.33228\n",
            "epochs [53/128] train_loss: 2.92744 val_loss: 0.33541\n",
            "epochs [54/128] train_loss: 2.88674 val_loss: 0.35138\n",
            "epochs [55/128] train_loss: 2.88338 val_loss: 0.35338\n",
            "epochs [56/128] train_loss: 2.89079 val_loss: 0.33678\n",
            "epochs [57/128] train_loss: 2.86189 val_loss: 0.34133\n",
            "epochs [58/128] train_loss: 2.87291 val_loss: 0.35276\n",
            "epochs [59/128] train_loss: 2.81497 val_loss: 0.35130\n",
            "epochs [60/128] train_loss: 2.83807 val_loss: 0.32911\n",
            "epochs [61/128] train_loss: 2.79177 val_loss: 0.38151\n",
            "epochs [62/128] train_loss: 2.97529 val_loss: 0.33177\n",
            "epochs [63/128] train_loss: 2.85393 val_loss: 0.33996\n",
            "epochs [64/128] train_loss: 2.78345 val_loss: 0.34650\n",
            "epochs [65/128] train_loss: 2.78387 val_loss: 0.31904\n",
            "epochs [66/128] train_loss: 2.78530 val_loss: 0.31890\n",
            "epochs [67/128] train_loss: 2.73440 val_loss: 0.31353\n",
            "epochs [68/128] train_loss: 2.86329 val_loss: 0.31969\n",
            "epochs [69/128] train_loss: 2.76054 val_loss: 0.33786\n",
            "epochs [70/128] train_loss: 2.72169 val_loss: 0.32887\n",
            "epochs [71/128] train_loss: 2.75082 val_loss: 0.33462\n",
            "epochs [72/128] train_loss: 2.68254 val_loss: 0.36457\n",
            "epochs [73/128] train_loss: 2.72046 val_loss: 0.31720\n",
            "epochs [74/128] train_loss: 2.74513 val_loss: 0.30971\n",
            "epochs [75/128] train_loss: 2.66197 val_loss: 0.32835\n",
            "epochs [76/128] train_loss: 2.69519 val_loss: 0.29414\n",
            "epochs [77/128] train_loss: 2.86505 val_loss: 0.31824\n",
            "epochs [78/128] train_loss: 2.64978 val_loss: 0.32191\n",
            "epochs [79/128] train_loss: 2.68995 val_loss: 0.31979\n",
            "epochs [80/128] train_loss: 2.63018 val_loss: 0.29772\n",
            "epochs [81/128] train_loss: 2.70949 val_loss: 0.31271\n",
            "epochs [82/128] train_loss: 2.65227 val_loss: 0.29496\n",
            "epochs [83/128] train_loss: 2.63039 val_loss: 0.32565\n",
            "epochs [84/128] train_loss: 2.64726 val_loss: 0.30855\n",
            "epochs [85/128] train_loss: 2.62687 val_loss: 0.29581\n",
            "epochs [86/128] train_loss: 2.62296 val_loss: 0.32122\n",
            "epochs [87/128] train_loss: 2.61131 val_loss: 0.31884\n",
            "epochs [88/128] train_loss: 2.60980 val_loss: 0.30213\n",
            "epochs [89/128] train_loss: 2.49324 val_loss: 0.33937\n",
            "epochs [90/128] train_loss: 2.53307 val_loss: 0.38148\n",
            "epochs [91/128] train_loss: 2.59514 val_loss: 0.28399\n",
            "epochs [92/128] train_loss: 2.49534 val_loss: 0.30677\n",
            "epochs [93/128] train_loss: 2.52757 val_loss: 0.30339\n",
            "epochs [94/128] train_loss: 2.48061 val_loss: 0.31196\n",
            "epochs [95/128] train_loss: 2.48236 val_loss: 0.28884\n",
            "epochs [96/128] train_loss: 2.50923 val_loss: 0.28252\n",
            "epochs [97/128] train_loss: 2.45918 val_loss: 0.35523\n",
            "epochs [98/128] train_loss: 2.53303 val_loss: 0.29417\n",
            "epochs [99/128] train_loss: 2.45754 val_loss: 0.26642\n",
            "epochs [100/128] train_loss: 2.45767 val_loss: 0.29016\n",
            "epochs [101/128] train_loss: 2.45033 val_loss: 0.28138\n",
            "epochs [102/128] train_loss: 2.39169 val_loss: 0.30545\n",
            "epochs [103/128] train_loss: 2.42829 val_loss: 0.29184\n",
            "epochs [104/128] train_loss: 2.45502 val_loss: 0.28931\n",
            "epochs [105/128] train_loss: 2.39060 val_loss: 0.28188\n",
            "epochs [106/128] train_loss: 2.39156 val_loss: 0.27606\n",
            "epochs [107/128] train_loss: 2.32309 val_loss: 0.31650\n",
            "epochs [108/128] train_loss: 2.36164 val_loss: 0.25716\n",
            "epochs [109/128] train_loss: 2.34123 val_loss: 0.27682\n",
            "epochs [110/128] train_loss: 2.36133 val_loss: 0.32367\n",
            "epochs [111/128] train_loss: 2.35959 val_loss: 0.25301\n",
            "epochs [112/128] train_loss: 2.33207 val_loss: 0.30491\n",
            "epochs [113/128] train_loss: 2.37430 val_loss: 0.28000\n",
            "epochs [114/128] train_loss: 2.28807 val_loss: 0.30360\n",
            "epochs [115/128] train_loss: 2.31946 val_loss: 0.28114\n",
            "epochs [116/128] train_loss: 2.32380 val_loss: 0.27590\n",
            "epochs [117/128] train_loss: 2.30688 val_loss: 0.27916\n",
            "epochs [118/128] train_loss: 2.30907 val_loss: 0.28335\n",
            "epochs [119/128] train_loss: 2.26093 val_loss: 0.26822\n",
            "epochs [120/128] train_loss: 2.24033 val_loss: 0.26869\n",
            "epochs [121/128] train_loss: 2.27154 val_loss: 0.25356\n",
            "epochs [122/128] train_loss: 2.31984 val_loss: 0.26046\n",
            "epochs [123/128] train_loss: 2.21034 val_loss: 0.29608\n",
            "epochs [124/128] train_loss: 2.28177 val_loss: 0.29276\n",
            "epochs [125/128] train_loss: 2.26532 val_loss: 0.24130\n",
            "epochs [126/128] train_loss: 2.24705 val_loss: 0.24147\n",
            "epochs [127/128] train_loss: 2.16317 val_loss: 0.25217\n",
            "epochs [128/128] train_loss: 2.15678 val_loss: 0.25753\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "full_train_index = [*range(0, len(X_wide_train))]\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
        "    train_dataset = torch.utils.data.DataLoader(\n",
        "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
        "                                                 batch_size=batch_size,)\n",
        "    val_dataset = torch.utils.data.DataLoader(\n",
        "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
        "                                                 batch_size=batch_size,)\n",
        "    # training\n",
        "    total_train_loss = 0\n",
        "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
        "        if torch.cuda.is_available():\n",
        "            net.cuda()\n",
        "            X_wide_train_cuda = batch_x[0].float().cuda()\n",
        "            X_deep_train_cuda = batch_x[1].float().cuda()\n",
        "            y_train_cuda = batch_y.cuda()\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
        "        loss = criterion(pred_y, y_train_cuda)\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # validating\n",
        "    total_val_loss = 0\n",
        "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
        "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
        "            y_val_cuda = batch_val_y.cuda()\n",
        "        \n",
        "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
        "        val_loss = criterion(pred_y, y_val_cuda)\n",
        "        total_val_loss += val_loss.item()\n",
        "        # print statistics\n",
        "    if min_val_loss > total_val_loss:\n",
        "        torch.save(net.state_dict(), model_name)\n",
        "        min_val_loss = total_val_loss\n",
        "    print('epochs [%d/%d] train_loss: %.5f val_loss: %.5f' % (epoch + 1, epochs, total_train_loss, total_val_loss))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EP8CukJ6QiP"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddKE7j0u6QiP"
      },
      "outputs": [],
      "source": [
        "net.load_state_dict(torch.load('/content/'+model_name))\n",
        "\n",
        "\n",
        "years = test[4].unique()\n",
        "\n",
        "test_list = []\n",
        "\n",
        "for year in years:\n",
        "    temp = test[test[4]==year]\n",
        "    temp = temp.reset_index(drop=True)\n",
        "    test_list.append(temp)\n",
        "    \n",
        "len(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g7mN-yR6QiR"
      },
      "outputs": [],
      "source": [
        "total=0\n",
        "with torch.no_grad():\n",
        "    for year, _test in zip(years, test_list):\n",
        "\n",
        "        print(year, '年:')\n",
        "        y_test = _test.loc[:,3]\n",
        "        X_wide_test = X_wide_scaler.transform(_test.loc[:,5:])\n",
        "\n",
        "        final_test_u_list = []\n",
        "        for ahead_time in ahead_times:\n",
        "            year_test_list = []\n",
        "            for pressure in pressures:\n",
        "                scaler_name = 'u' +str(pressure) + str(ahead_time)\n",
        "                X_deep = reanalysis_u_test_dict[scaler_name][reanalysis_u_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
        "                X_deep = X_deep_u_scaler_dict[scaler_name].transform(X_deep)\n",
        "                X_deep_final = X_deep.reshape(-1, 1, 1, 3, 3, 1)\n",
        "                year_test_list.append(X_deep_final)\n",
        "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
        "            final_test_u_list.append(X_deep_temp)\n",
        "        X_deep_u_test = np.concatenate(final_test_u_list, axis=5)\n",
        "        \n",
        "        final_test_v_list = []\n",
        "        for ahead_time in ahead_times:\n",
        "            year_test_list = []\n",
        "            for pressure in pressures:\n",
        "                scaler_name = 'v' +str(pressure) + str(ahead_time)\n",
        "                X_deep = reanalysis_v_test_dict[scaler_name][reanalysis_v_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
        "                X_deep = X_deep_v_scaler_dict[scaler_name].transform(X_deep)\n",
        "                X_deep_final = X_deep.reshape(-1, 1, 1, 3, 3, 1)\n",
        "                year_test_list.append(X_deep_final)\n",
        "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
        "            final_test_v_list.append(X_deep_temp)\n",
        "        X_deep_v_test = np.concatenate(final_test_v_list, axis=5)\n",
        "\n",
        "        final_test_t_list = []\n",
        "        for ahead_time in ahead_times:\n",
        "            year_test_list = []\n",
        "            for pressure in pressures:\n",
        "                scaler_name = 't' +str(pressure) + str(ahead_time)\n",
        "                X_deep = reanalysis_t_test_dict[scaler_name][reanalysis_t_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
        "                X_deep = X_deep_t_scaler_dict[scaler_name].transform(X_deep)\n",
        "                X_deep_final = X_deep.reshape(-1, 1, 1, 3, 3, 1)\n",
        "                year_test_list.append(X_deep_final)\n",
        "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
        "            final_test_t_list.append(X_deep_temp)\n",
        "        X_deep_t_test = np.concatenate(final_test_t_list, axis=5)\n",
        "\n",
        "        final_test_r_list = []\n",
        "        for ahead_time in ahead_times:\n",
        "            year_test_list = []\n",
        "            for pressure in pressures:\n",
        "                scaler_name = 'r' +str(pressure) + str(ahead_time)\n",
        "                X_deep = reanalysis_r_test_dict[scaler_name][reanalysis_r_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
        "                X_deep = X_deep_r_scaler_dict[scaler_name].transform(X_deep)\n",
        "                X_deep_final = X_deep.reshape(-1, 1, 1, 3, 3, 1)\n",
        "                year_test_list.append(X_deep_final)\n",
        "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
        "            final_test_r_list.append(X_deep_temp)\n",
        "        X_deep_r_test = np.concatenate(final_test_r_list, axis=5)\n",
        "\n",
        "    \n",
        "        X_deep_test = np.concatenate((X_deep_u_test, X_deep_v_test,X_deep_t_test,X_deep_r_test), axis=1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
        "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
        "\n",
        "        pred = net(X_wide_test, X_deep_test)\n",
        "\n",
        "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy().reshape(-1,1))\n",
        "        true = y_test.values.reshape(-1, 1)\n",
        "        diff = np.abs(pred - true)\n",
        "\n",
        "        print('avg wind error:', sum(diff)/len(diff))\n",
        "        total=total+sum(diff)/len(diff)\n",
        "print(\"The mean avg is::\",total/len(test_list))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "o3LytQxr6QiE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}