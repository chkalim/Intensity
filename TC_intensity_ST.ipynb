{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "0HQ-TWHAnhIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd97ffd-16f0-481b-a656-401355f3f63f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "4a9jYmIv6Qhr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as Data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "import random\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzXyQ_ii6Qhx"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "8wEqM_y46Qh0"
      },
      "outputs": [],
      "source": [
        "#  predition TI of leading time at 24 hours\n",
        "pre_seq = 4\n",
        "batch_size = 128\n",
        "epochs = 128\n",
        "min_val_loss = 100\n",
        "model_name = 'fusion_model_save.pkl'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "wGMIKq336Qh1"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/intensity/TI_Prediction-master/data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
        "test= pd.read_csv('/content/drive/My Drive/Colab Notebooks/intensity/TI_Prediction-master/data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "Q3mRf70Q6Qh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e74e2ee-b76f-41fd-95d1-2b8fcff84575"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8406, 101), (2747, 101))"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ],
      "source": [
        "train.shape, test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "crasYUyh6Qh5"
      },
      "outputs": [],
      "source": [
        "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
        "CLIPER_feature.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "t5Sja7ar6Qh7"
      },
      "outputs": [],
      "source": [
        "X_wide_scaler = MinMaxScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "\n",
        "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 5:])\n",
        "X_wide_train = X_wide[0: train.shape[0], :]\n",
        "\n",
        "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3].values.reshape(-1, 1))\n",
        "y_train = y[0: train.shape[0], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "Q-2NuhPY6Qh8"
      },
      "outputs": [],
      "source": [
        "\n",
        "ahead_times = [0]\n",
        "pressures = [1, 2, 3,5, 7, 10,20, 30, 50,70, 100, 125,150, 175, 200,225,\n",
        "             250, 300,350, 400, 450,500, 550, 600,650, 700, 750,775,\n",
        "             800, 825,850, 875, 900,925, 950, 975,1000]\n",
        "\n",
        "sequential_reanalysis_u_list = []\n",
        "reanalysis_u_test_dict = {}\n",
        "X_deep_u_scaler_dict = {}\n",
        "\n",
        "sequential_reanalysis_v_list = []\n",
        "reanalysis_v_test_dict = {}\n",
        "X_deep_v_scaler_dict = {}\n",
        "\n",
        "sequential_reanalysis_t_list = []\n",
        "reanalysis_t_test_dict = {}\n",
        "X_deep_t_scaler_dict = {}\n",
        "\n",
        "\n",
        "sequential_reanalysis_r_list = []\n",
        "reanalysis_r_test_dict = {}\n",
        "X_deep_r_scaler_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reanalysis_type = 'v'\n",
        "for ahead_time in ahead_times:\n",
        "    reanalysis_list = []\n",
        "    for pressure in pressures:\n",
        "        folder = None\n",
        "        if ahead_time == 0:\n",
        "            folder = reanalysis_type\n",
        "        else:\n",
        "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
        "        all_reanalysis_csv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/ERA_Interim/'+folder+'/reanalysis_'+reanalysis_type+'_'+str(pressure)+'.csv', header=None)\n",
        "\n",
        "        train_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(train[0].unique())]\n",
        "        test_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(test[0].unique())]\n",
        "        reanalysis_v_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
        "\n",
        "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
        "        reanalysis.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
        "        X_deep_v_scaler_dict[scaler_name] = MinMaxScaler()\n",
        "        \n",
        "        # 5:end is the 31*31 v component wind speed\n",
        "        X_deep = X_deep_v_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
        "        \n",
        "        # (batch, type, channel, height, widht, time) here type is v\n",
        "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 3, 3, 1)\n",
        "        reanalysis_list.append(X_deep_final)\n",
        "        \n",
        "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
        "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
        "    sequential_reanalysis_v_list.append(X_deep_temp)\n",
        "\n",
        "X_deep_v_train = np.concatenate(sequential_reanalysis_v_list, axis=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHW1Vzeivgv1",
        "outputId": "630ceee9-4a88-48c3-bd0c-e79f0ad9d259"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahead_time: 0 (8406, 1, 37, 3, 3, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-03T17:50:28.798167Z",
          "start_time": "2021-08-03T17:49:53.943725Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVu8oCxx6Qh-",
        "outputId": "86402b8a-f19f-4d32-850b-ac2acd89f5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahead_time: 0 (8406, 1, 37, 3, 3, 1)\n"
          ]
        }
      ],
      "source": [
        "reanalysis_type = 't'\n",
        "for ahead_time in ahead_times:\n",
        "    reanalysis_list = []\n",
        "    for pressure in pressures:\n",
        "        folder = None\n",
        "        if ahead_time == 0:\n",
        "            folder = reanalysis_type\n",
        "        else:\n",
        "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
        "\n",
        "        all_reanalysis_csv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/ERA_Interim/'+folder+'/reanalysis_'+reanalysis_type+'_'+str(pressure)+'.csv', header=None)\n",
        "       \n",
        "\n",
        "        train_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(train[0].unique())]\n",
        "        test_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(test[0].unique())]\n",
        "        reanalysis_t_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
        "        \n",
        "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
        "        reanalysis.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
        "        X_deep_t_scaler_dict[scaler_name] = MinMaxScaler()\n",
        "        \n",
        "        # 5:end is the 31*31 u component wind speed\n",
        "        X_deep = X_deep_t_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
        "        \n",
        "        # (batch, type, channel, height, widht, time) here type is u\n",
        "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 3, 3, 1)\n",
        "        reanalysis_list.append(X_deep_final)\n",
        "\n",
        "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
        "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
        "    sequential_reanalysis_t_list.append(X_deep_temp)\n",
        "\n",
        "X_deep_t_train = np.concatenate(sequential_reanalysis_t_list, axis=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-03T17:50:28.798167Z",
          "start_time": "2021-08-03T17:49:53.943725Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3256c6-af23-45a5-fe0b-1848301c80a0",
        "id": "yMX-BMc7W-oe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahead_time: 0 (8406, 1, 37, 3, 3, 1)\n"
          ]
        }
      ],
      "source": [
        "reanalysis_type = 'u'\n",
        "for ahead_time in ahead_times:\n",
        "    reanalysis_list = []\n",
        "    for pressure in pressures:\n",
        "        folder = None\n",
        "        if ahead_time == 0:\n",
        "            folder = reanalysis_type\n",
        "        else:\n",
        "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
        "\n",
        "        all_reanalysis_csv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/ERA_Interim/'+folder+'/reanalysis_'+reanalysis_type+'_'+str(pressure)+'.csv', header=None)\n",
        "      \n",
        "\n",
        "        train_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(train[0].unique())]\n",
        "        test_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(test[0].unique())]\n",
        "        reanalysis_u_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
        "        \n",
        "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
        "        reanalysis.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
        "        X_deep_u_scaler_dict[scaler_name] = MinMaxScaler()\n",
        "        \n",
        "        # 5:end is the 31*31 u component wind speed\n",
        "        X_deep = X_deep_u_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
        "        \n",
        "        # (batch, type, channel, height, widht, time) here type is u\n",
        "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 3, 3, 1)\n",
        "        reanalysis_list.append(X_deep_final)\n",
        "\n",
        "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
        "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
        "    sequential_reanalysis_u_list.append(X_deep_temp)\n",
        "\n",
        "X_deep_u_train = np.concatenate(sequential_reanalysis_u_list, axis=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-03T17:50:28.798167Z",
          "start_time": "2021-08-03T17:49:53.943725Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227f0161-489e-477d-8818-1178e0ec2357",
        "id": "PWn467wDXCWS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahead_time: 0 (8406, 1, 37, 3, 3, 1)\n"
          ]
        }
      ],
      "source": [
        "reanalysis_type = 'r'\n",
        "for ahead_time in ahead_times:\n",
        "    reanalysis_list = []\n",
        "    for pressure in pressures:\n",
        "        folder = None\n",
        "        if ahead_time == 0:\n",
        "            folder = reanalysis_type\n",
        "        else:\n",
        "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
        "\n",
        "        all_reanalysis_csv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/typhoon/data/ERA_Interim/'+folder+'/reanalysis_'+reanalysis_type+'_'+str(pressure)+'.csv', header=None)\n",
        "       \n",
        "\n",
        "        train_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(train[0].unique())]\n",
        "        test_reanalysis = all_reanalysis_csv[all_reanalysis_csv[0].isin(test[0].unique())]\n",
        "        reanalysis_r_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
        "        \n",
        "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
        "        reanalysis.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
        "        X_deep_r_scaler_dict[scaler_name] = MinMaxScaler()\n",
        "        \n",
        "        # 5:end is the 31*31 u component wind speed\n",
        "        X_deep = X_deep_r_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
        "        \n",
        "        # (batch, type, channel, height, widht, time) here type is u\n",
        "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 3, 3, 1)\n",
        "        reanalysis_list.append(X_deep_final)\n",
        "\n",
        "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
        "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
        "    sequential_reanalysis_r_list.append(X_deep_temp)\n",
        "\n",
        "X_deep_r_train = np.concatenate(sequential_reanalysis_r_list, axis=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "01K0MPdV6QiC"
      },
      "outputs": [],
      "source": [
        "# X_deep_train = np.concatenate((X_deep_u_train, X_deep_v_train), axis=1)\n",
        "X_deep_train = np.concatenate((X_deep_u_train, X_deep_v_train,X_deep_t_train,X_deep_r_train), axis=1)\n",
        "# X_deep_train = X_deep_u_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "0TKeiori6QiD",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cfd8a76-d71b-46d9-a837-075a4ab5af8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8406, 96), (8406, 4, 37, 3, 3, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 306
        }
      ],
      "source": [
        "X_wide_train.shape, X_deep_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3LytQxr6QiE"
      },
      "source": [
        "# training set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "y32yiRNg6QiE"
      },
      "outputs": [],
      "source": [
        "class TrainLoader(Data.Dataset):\n",
        "    def __init__(self, X_wide_train, X_deep_train, y_train):\n",
        "        self.X_wide_train = X_wide_train\n",
        "        self.X_deep_train = X_deep_train\n",
        "        self.y_train = y_train\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return [self.X_wide_train[index], self.X_deep_train[index]], self.y_train[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X_wide_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "sPODW5Nf6QiG"
      },
      "outputs": [],
      "source": [
        "full_train_index = [*range(0, len(X_wide_train))]\n",
        "\n",
        "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "hPCfXwOQ6QiG",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afab180a-c92e-4ce8-8417-a779b3c7e47a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7565, 841)"
            ]
          },
          "metadata": {},
          "execution_count": 309
        }
      ],
      "source": [
        "len(train_index), len(val_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "GX4zv_UZ6QiG"
      },
      "outputs": [],
      "source": [
        "train_dataset = torch.utils.data.DataLoader(\n",
        "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
        "                                                 batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "TpD4FI4x6QiI"
      },
      "outputs": [],
      "source": [
        "val_dataset = torch.utils.data.DataLoader(\n",
        "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
        "                                                 batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRTsIwMu6QiJ"
      },
      "source": [
        "# proposed Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "u9xd8v1Z1r8U"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "\n",
        "\n",
        "        # spatial attention block for checking the results of SAF-Net model. Not related to the proposed model\n",
        "        self.att_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.att_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.att_block_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "        self.cross_unit = nn.Parameter(data=torch.ones(len(ahead_times), 6))\n",
        "        # fuse\n",
        "        self.fuse_unit = nn.Parameter(data=torch.ones(len(ahead_times), 4))\n",
        "        \n",
        "        # End  code for the results of the SAF-NET model\n",
        "\n",
        "        # deep\n",
        "        self.conv1 = nn.Conv2d(37, 64, kernel_size=1, padding=(1, 1))\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=1, padding=(1, 1))\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=1, padding=(1, 1))\n",
        "        self.fc1 = nn.Linear(256 *2 *2, 128)\n",
        "\n",
        "        self.fc2 = nn.Linear(96 + 128 * len(ahead_times), 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.fc4=nn.Linear(128,1)\n",
        "\n",
        "        self.num_layers=1\n",
        "        self.hidden_size=128\n",
        "        self.gru = nn.GRU(input_size=96, hidden_size=128,\n",
        "                            num_layers=1, batch_first=True)\n",
        "        self.gru2 = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers, batch_first=True)\n",
        "        self.gru3 = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers, batch_first=True)\n",
        "        self.gru4 = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, wide, deep):\n",
        "        seq_list = []\n",
        "        seq_list2 = []\n",
        "        seq_list3 = []\n",
        "        seq_list4 = []\n",
        "\n",
        "\n",
        "# Temporal section of the model\n",
        "        x = wide.unsqueeze(1)\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size,device=x.device).float())\n",
        "        h_1 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size,device=x.device).float())\n",
        "        h_2 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size,device=x.device).float())\n",
        "        h_3 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size,device=x.device).float())\n",
        "        ula,_  = self.gru(x, h_0)\n",
        "        ula2, _ = self.gru2(ula,h_1)\n",
        "        ula3, _ = self.gru3(ula2, h_2)\n",
        "        ula4, _ = self.gru4(ula3, h_3)\n",
        "        h_out = ula4.view(-1, self.hidden_size)\n",
        "\n",
        "        # output of the temporal model\n",
        "        h_out = self.fc4(h_out)\n",
        "\n",
        "# spatial section of the model\n",
        "        for i in range(len(ahead_times)):\n",
        "            \n",
        "            deep_u = deep[:,0,:,:,:,i]\n",
        "            deep_v = deep[:,1,:,:,:,i]\n",
        "            deep_t = deep[:,2,:,:,:,i]\n",
        "            deep_r = deep[:,3,:,:,:,i]\n",
        "\n",
        "          # First Convolutional layer  \n",
        "            deep_u = self.pool(F.relu(self.conv1(deep_u)))\n",
        "            deep_v = self.pool(F.relu(self.conv1(deep_v)))\n",
        "            deep_t = self.pool(F.relu(self.conv1(deep_t)))\n",
        "            deep_r = self.pool(F.relu(self.conv1(deep_r)))\n",
        "\n",
        "          # 2nd Convolutional layer  \n",
        "            deep_u = self.pool(F.relu(self.conv2(deep_u))) \n",
        "            deep_v = self.pool(F.relu(self.conv2(deep_v)))\n",
        "            deep_t = self.pool(F.relu(self.conv2(deep_t))) \n",
        "            deep_r = self.pool(F.relu(self.conv2(deep_r)))\n",
        "\n",
        "\n",
        "           # 3rd Convolutional layer  \n",
        "            deep_u = self.pool(F.relu(self.conv3(deep_u)))\n",
        "            deep_v = self.pool(F.relu(self.conv3(deep_v)))\n",
        "            deep_t = self.pool(F.relu(self.conv3(deep_t)))\n",
        "            deep_r = self.pool(F.relu(self.conv3(deep_r)))\n",
        "\n",
        "\n",
        "          #  fully connected layer\n",
        "            time_seq = deep_u.view(-1, 256 * 2 * 2)\n",
        "            time_seq2 = deep_v.view(-1, 256 * 2 * 2)\n",
        "            time_seq3 = deep_t.view(-1, 256 * 2 * 2)\n",
        "            time_seq4 = deep_r.view(-1, 256 * 2 * 2)\n",
        "\n",
        "            time_seq = self.fc1(time_seq)\n",
        "            time_seq2 = self.fc1(time_seq2)\n",
        "            time_seq3 = self.fc1(time_seq3)\n",
        "            time_seq4 = self.fc1(time_seq4)\n",
        "\n",
        "\n",
        "            seq_list.append(time_seq)\n",
        "            seq_list2.append(time_seq2)\n",
        "            seq_list3.append(time_seq3)\n",
        "            seq_list4.append(time_seq4)\n",
        "\n",
        "#  concatination of temporal functions with each spatial components\n",
        "        wide = wide.view(-1, 96)\n",
        "        wide_n_deep_u = torch.cat((wide, seq_list[0]),1)\n",
        "        if len(ahead_times) > 1:\n",
        "            for i in range(1, len(ahead_times)):\n",
        "                wide_n_deep = torch.cat((wide_n_deep_u, seq_list[i]),1)\n",
        "        wide_n_deep_u = F.relu(self.fc2(wide_n_deep_u))\n",
        "        # individual output of the u component along with temporal features\n",
        "        wide_n_deep_u = F.relu(self.fc3(wide_n_deep_u))\n",
        "\n",
        "        wide_n_deep_v = torch.cat((wide, seq_list2[0]),1)\n",
        "        if len(ahead_times) > 1:\n",
        "            for i in range(1, len(ahead_times)):\n",
        "                wide_n_deep_v = torch.cat((wide_n_deep_v, seq_list2[i]),1)\n",
        "        wide_n_deep_v = F.relu(self.fc2(wide_n_deep_v))\n",
        "        # individual output of the v component along with temporal features\n",
        "        wide_n_deep_v = F.relu(self.fc3(wide_n_deep_v))\n",
        "\n",
        "\n",
        "        wide_n_deep_t = torch.cat((wide, seq_list3[0]),1)\n",
        "        if len(ahead_times) > 1:\n",
        "            for i in range(1, len(ahead_times)):\n",
        "                wide_n_deep_t = torch.cat((wide_n_deep_t, seq_list3[i]),1)\n",
        "        wide_n_deep_t = F.relu(self.fc2(wide_n_deep_t))\n",
        "        # individual output of the t component along with temporal features\n",
        "        wide_n_deep_t = F.relu(self.fc3(wide_n_deep_t))\n",
        "\n",
        "        wide_n_deep_r = torch.cat((wide, seq_list4[0]),1)\n",
        "        if len(ahead_times) > 1:\n",
        "            for i in range(1, len(ahead_times)):\n",
        "                wide_n_deep_r = torch.cat((wide_n_deep_r, seq_list4[i]),1)\n",
        "        wide_n_deep_r = F.relu(self.fc2(wide_n_deep_r))\n",
        "        # individual output of the r component along with temporal features\n",
        "        wide_n_deep_r = F.relu(self.fc3(wide_n_deep_r))\n",
        "\n",
        "        return (wide_n_deep_u+wide_n_deep_v+wide_n_deep_t+wide_n_deep_r+h_out)/5\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "xGjSttah6QiM"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rovVUEpc6QiN"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "asdakmwC6QiN"
      },
      "outputs": [],
      "source": [
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.0007)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "ot5O0ZX151-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d881e37d-ce8f-4cc9-9682-3b97d92e275b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(device=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "8-31895G6QiO",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6252f94-b47a-4b22-f8aa-bb66c91a0189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs [1/128] train_loss: 7.04403 val_loss: 0.59293\n",
            "epochs [2/128] train_loss: 4.51763 val_loss: 0.55036\n",
            "epochs [3/128] train_loss: 4.33862 val_loss: 0.47606\n",
            "epochs [4/128] train_loss: 4.22913 val_loss: 0.47789\n",
            "epochs [5/128] train_loss: 4.04308 val_loss: 0.48166\n",
            "epochs [6/128] train_loss: 3.90792 val_loss: 0.41974\n",
            "epochs [7/128] train_loss: 3.76421 val_loss: 0.46776\n",
            "epochs [8/128] train_loss: 3.77415 val_loss: 0.39225\n",
            "epochs [9/128] train_loss: 3.72001 val_loss: 0.42875\n",
            "epochs [10/128] train_loss: 3.64474 val_loss: 0.43390\n",
            "epochs [11/128] train_loss: 3.63440 val_loss: 0.47632\n",
            "epochs [12/128] train_loss: 3.56930 val_loss: 0.48071\n",
            "epochs [13/128] train_loss: 3.54215 val_loss: 0.40022\n",
            "epochs [14/128] train_loss: 3.46047 val_loss: 0.43455\n",
            "epochs [15/128] train_loss: 3.53355 val_loss: 0.41113\n",
            "epochs [16/128] train_loss: 3.52696 val_loss: 0.40971\n",
            "epochs [17/128] train_loss: 3.44511 val_loss: 0.38878\n",
            "epochs [18/128] train_loss: 3.34791 val_loss: 0.38372\n",
            "epochs [19/128] train_loss: 3.37225 val_loss: 0.38686\n",
            "epochs [20/128] train_loss: 3.28850 val_loss: 0.44366\n",
            "epochs [21/128] train_loss: 3.33037 val_loss: 0.38526\n",
            "epochs [22/128] train_loss: 3.30616 val_loss: 0.39382\n",
            "epochs [23/128] train_loss: 3.25815 val_loss: 0.41212\n",
            "epochs [24/128] train_loss: 3.27619 val_loss: 0.37269\n",
            "epochs [25/128] train_loss: 3.21821 val_loss: 0.36350\n",
            "epochs [26/128] train_loss: 3.20883 val_loss: 0.39117\n",
            "epochs [27/128] train_loss: 3.22343 val_loss: 0.38726\n",
            "epochs [28/128] train_loss: 3.19963 val_loss: 0.37269\n",
            "epochs [29/128] train_loss: 3.18394 val_loss: 0.37045\n",
            "epochs [30/128] train_loss: 3.11384 val_loss: 0.38042\n",
            "epochs [31/128] train_loss: 3.10750 val_loss: 0.39409\n",
            "epochs [32/128] train_loss: 3.15478 val_loss: 0.38444\n",
            "epochs [33/128] train_loss: 3.09560 val_loss: 0.37350\n",
            "epochs [34/128] train_loss: 3.15767 val_loss: 0.37103\n",
            "epochs [35/128] train_loss: 3.15422 val_loss: 0.36943\n",
            "epochs [36/128] train_loss: 3.13841 val_loss: 0.38373\n",
            "epochs [37/128] train_loss: 3.07797 val_loss: 0.33835\n",
            "epochs [38/128] train_loss: 3.06454 val_loss: 0.36683\n",
            "epochs [39/128] train_loss: 3.02541 val_loss: 0.34348\n",
            "epochs [40/128] train_loss: 3.04868 val_loss: 0.37341\n",
            "epochs [41/128] train_loss: 3.01469 val_loss: 0.35105\n",
            "epochs [42/128] train_loss: 2.98122 val_loss: 0.35241\n",
            "epochs [43/128] train_loss: 3.00909 val_loss: 0.32785\n",
            "epochs [44/128] train_loss: 2.99267 val_loss: 0.33291\n",
            "epochs [45/128] train_loss: 2.95420 val_loss: 0.37505\n",
            "epochs [46/128] train_loss: 3.07258 val_loss: 0.33909\n",
            "epochs [47/128] train_loss: 2.98240 val_loss: 0.37112\n",
            "epochs [48/128] train_loss: 2.98458 val_loss: 0.35551\n",
            "epochs [49/128] train_loss: 2.95169 val_loss: 0.38360\n",
            "epochs [50/128] train_loss: 3.02143 val_loss: 0.34565\n",
            "epochs [51/128] train_loss: 2.94105 val_loss: 0.33335\n",
            "epochs [52/128] train_loss: 2.90042 val_loss: 0.34954\n",
            "epochs [53/128] train_loss: 2.93025 val_loss: 0.36572\n",
            "epochs [54/128] train_loss: 2.91325 val_loss: 0.34931\n",
            "epochs [55/128] train_loss: 2.92178 val_loss: 0.36086\n",
            "epochs [56/128] train_loss: 2.96816 val_loss: 0.36415\n",
            "epochs [57/128] train_loss: 2.89654 val_loss: 0.31723\n",
            "epochs [58/128] train_loss: 2.85331 val_loss: 0.32257\n",
            "epochs [59/128] train_loss: 2.88113 val_loss: 0.34178\n",
            "epochs [60/128] train_loss: 2.83293 val_loss: 0.34723\n",
            "epochs [61/128] train_loss: 2.85489 val_loss: 0.32339\n",
            "epochs [62/128] train_loss: 2.84077 val_loss: 0.32350\n",
            "epochs [63/128] train_loss: 2.77354 val_loss: 0.33710\n",
            "epochs [64/128] train_loss: 2.79733 val_loss: 0.34761\n",
            "epochs [65/128] train_loss: 2.81683 val_loss: 0.31430\n",
            "epochs [66/128] train_loss: 2.78957 val_loss: 0.33089\n",
            "epochs [67/128] train_loss: 2.78228 val_loss: 0.33313\n",
            "epochs [68/128] train_loss: 2.75881 val_loss: 0.33803\n",
            "epochs [69/128] train_loss: 2.81131 val_loss: 0.34278\n",
            "epochs [70/128] train_loss: 2.74675 val_loss: 0.33131\n",
            "epochs [71/128] train_loss: 2.77447 val_loss: 0.30989\n",
            "epochs [72/128] train_loss: 2.71792 val_loss: 0.30674\n",
            "epochs [73/128] train_loss: 2.66940 val_loss: 0.31697\n",
            "epochs [74/128] train_loss: 2.64933 val_loss: 0.34094\n",
            "epochs [75/128] train_loss: 2.77965 val_loss: 0.30505\n",
            "epochs [76/128] train_loss: 2.70042 val_loss: 0.32535\n",
            "epochs [77/128] train_loss: 2.71177 val_loss: 0.32407\n",
            "epochs [78/128] train_loss: 2.66983 val_loss: 0.30038\n",
            "epochs [79/128] train_loss: 2.61142 val_loss: 0.30912\n",
            "epochs [80/128] train_loss: 2.59547 val_loss: 0.33936\n",
            "epochs [81/128] train_loss: 2.59276 val_loss: 0.29907\n",
            "epochs [82/128] train_loss: 2.67951 val_loss: 0.31239\n",
            "epochs [83/128] train_loss: 2.60624 val_loss: 0.32149\n",
            "epochs [84/128] train_loss: 2.68488 val_loss: 0.28704\n",
            "epochs [85/128] train_loss: 2.57260 val_loss: 0.34861\n",
            "epochs [86/128] train_loss: 2.67715 val_loss: 0.30742\n",
            "epochs [87/128] train_loss: 2.53842 val_loss: 0.31579\n",
            "epochs [88/128] train_loss: 2.56972 val_loss: 0.31708\n",
            "epochs [89/128] train_loss: 2.54802 val_loss: 0.29062\n",
            "epochs [90/128] train_loss: 2.52364 val_loss: 0.29568\n",
            "epochs [91/128] train_loss: 2.49444 val_loss: 0.29465\n",
            "epochs [92/128] train_loss: 2.53750 val_loss: 0.29621\n",
            "epochs [93/128] train_loss: 2.52618 val_loss: 0.27682\n",
            "epochs [94/128] train_loss: 2.56055 val_loss: 0.30128\n",
            "epochs [95/128] train_loss: 2.51486 val_loss: 0.28079\n",
            "epochs [96/128] train_loss: 2.52722 val_loss: 0.28173\n",
            "epochs [97/128] train_loss: 2.49968 val_loss: 0.28846\n",
            "epochs [98/128] train_loss: 2.56691 val_loss: 0.31721\n",
            "epochs [99/128] train_loss: 2.43695 val_loss: 0.30484\n",
            "epochs [100/128] train_loss: 2.54341 val_loss: 0.27849\n",
            "epochs [101/128] train_loss: 2.51024 val_loss: 0.28088\n",
            "epochs [102/128] train_loss: 2.39853 val_loss: 0.27539\n",
            "epochs [103/128] train_loss: 2.39373 val_loss: 0.28801\n",
            "epochs [104/128] train_loss: 2.40009 val_loss: 0.28500\n",
            "epochs [105/128] train_loss: 2.34450 val_loss: 0.29102\n",
            "epochs [106/128] train_loss: 2.46049 val_loss: 0.27722\n",
            "epochs [107/128] train_loss: 2.33092 val_loss: 0.27364\n",
            "epochs [108/128] train_loss: 2.33207 val_loss: 0.26790\n",
            "epochs [109/128] train_loss: 2.33426 val_loss: 0.27599\n",
            "epochs [110/128] train_loss: 2.31618 val_loss: 0.28271\n",
            "epochs [111/128] train_loss: 2.37290 val_loss: 0.25664\n",
            "epochs [112/128] train_loss: 2.31467 val_loss: 0.27658\n",
            "epochs [113/128] train_loss: 2.30622 val_loss: 0.26434\n",
            "epochs [114/128] train_loss: 2.27528 val_loss: 0.26315\n",
            "epochs [115/128] train_loss: 2.26205 val_loss: 0.27961\n",
            "epochs [116/128] train_loss: 2.28287 val_loss: 0.25957\n",
            "epochs [117/128] train_loss: 2.25823 val_loss: 0.26957\n",
            "epochs [118/128] train_loss: 2.26306 val_loss: 0.26221\n",
            "epochs [119/128] train_loss: 2.27977 val_loss: 0.24436\n",
            "epochs [120/128] train_loss: 2.21578 val_loss: 0.27561\n",
            "epochs [121/128] train_loss: 2.23845 val_loss: 0.27408\n",
            "epochs [122/128] train_loss: 2.18454 val_loss: 0.26108\n",
            "epochs [123/128] train_loss: 2.34653 val_loss: 0.25614\n",
            "epochs [124/128] train_loss: 2.21094 val_loss: 0.25579\n",
            "epochs [125/128] train_loss: 2.18414 val_loss: 0.26170\n",
            "epochs [126/128] train_loss: 2.15352 val_loss: 0.25704\n",
            "epochs [127/128] train_loss: 2.16612 val_loss: 0.29266\n",
            "epochs [128/128] train_loss: 2.18283 val_loss: 0.26218\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "full_train_index = [*range(0, len(X_wide_train))]\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
        "    train_dataset = torch.utils.data.DataLoader(\n",
        "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
        "                                                 batch_size=batch_size,)\n",
        "    val_dataset = torch.utils.data.DataLoader(\n",
        "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
        "                                                 batch_size=batch_size,)\n",
        "    # training\n",
        "    total_train_loss = 0\n",
        "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
        "        if torch.cuda.is_available():\n",
        "            net.cuda()\n",
        "            X_wide_train_cuda = batch_x[0].float().cuda()\n",
        "            X_deep_train_cuda = batch_x[1].float().cuda()\n",
        "            y_train_cuda = batch_y.cuda()\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
        "        loss = criterion(pred_y, y_train_cuda)\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # validating\n",
        "    total_val_loss = 0\n",
        "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
        "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
        "            y_val_cuda = batch_val_y.cuda()\n",
        "        \n",
        "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
        "        val_loss = criterion(pred_y, y_val_cuda)\n",
        "        total_val_loss += val_loss.item()\n",
        "        # print statistics\n",
        "    if min_val_loss > total_val_loss:\n",
        "        torch.save(net.state_dict(), model_name)\n",
        "        min_val_loss = total_val_loss\n",
        "    print('epochs [%d/%d] train_loss: %.5f val_loss: %.5f' % (epoch + 1, epochs, total_train_loss, total_val_loss))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EP8CukJ6QiP"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "ddKE7j0u6QiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00dff8bb-8a8a-42a6-fd19-f80459289201"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 320
        }
      ],
      "source": [
        "# net.load_state_dict(torch.load('/content/'+model_name))\n",
        "net.load_state_dict(torch.load('/content/fusion_model_save.pkl'))\n",
        "years = test[4].unique()\n",
        "\n",
        "test_list = []\n",
        "\n",
        "for year in years:\n",
        "    temp = test[test[4]==year]\n",
        "    temp = temp.reset_index(drop=True)\n",
        "    test_list.append(temp)\n",
        "    \n",
        "len(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "0g7mN-yR6QiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8da3f1-4dc2-47df-e283-a0807390c11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2015 年:\n",
            "avg wind error: [4.09768065]\n",
            "2016 年:\n",
            "avg wind error: [5.082216]\n",
            "2017 年:\n",
            "avg wind error: [3.56589794]\n",
            "2018 年:\n",
            "avg wind error: [3.90197026]\n",
            "The mean avg is:: [4.16194121]\n"
          ]
        }
      ],
      "source": [
        "total=0\n",
        "with torch.no_grad():\n",
        "    for year, _test in zip(years, test_list):\n",
        "\n",
        "        print(year, '年:')\n",
        "        y_test = _test.loc[:,3]\n",
        "        X_wide_test = X_wide_scaler.transform(_test.loc[:,5:])\n",
        "\n",
        "        final_test_u_list = []\n",
        "        for ahead_time in ahead_times:\n",
        "            year_test_list = []\n",
        "            for pressure in pressures:\n",
        "                scaler_name = 'u' +str(pressure) + str(ahead_time)\n",
        "                X_deep = reanalysis_u_test_dict[scaler_name][reanalysis_u_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
        "                X_deep = X_deep_u_scaler_dict[scaler_name].transform(X_deep)\n",
        "                X_deep_final = X_deep.reshape(-1, 1, 1, 3, 3, 1)\n",
        "                year_test_list.append(X_deep_final)\n",
        "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
        "            final_test_u_list.append(X_deep_temp)\n",
        "        X_deep_u_test = np.concatenate(final_test_u_list, axis=5)\n",
        "        \n",
        "        final_test_v_list = []\n",
        "        for ahead_time in ahead_times:\n",
        "            year_test_list = []\n",
        "            for pressure in pressures:\n",
        "                scaler_name = 'v' +str(pressure) + str(ahead_time)\n",
        "                X_deep = reanalysis_v_test_dict[scaler_name][reanalysis_v_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
        "                X_deep = X_deep_v_scaler_dict[scaler_name].transform(X_deep)\n",
        "                X_deep_final = X_deep.reshape(-1, 1, 1, 3, 3, 1)\n",
        "                year_test_list.append(X_deep_final)\n",
        "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
        "            final_test_v_list.append(X_deep_temp)\n",
        "        X_deep_v_test = np.concatenate(final_test_v_list, axis=5)\n",
        "\n",
        "        final_test_t_list = []\n",
        "        for ahead_time in ahead_times:\n",
        "            year_test_list = []\n",
        "            for pressure in pressures:\n",
        "                scaler_name = 't' +str(pressure) + str(ahead_time)\n",
        "                X_deep = reanalysis_t_test_dict[scaler_name][reanalysis_t_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
        "                X_deep = X_deep_t_scaler_dict[scaler_name].transform(X_deep)\n",
        "                X_deep_final = X_deep.reshape(-1, 1, 1, 3, 3, 1)\n",
        "                year_test_list.append(X_deep_final)\n",
        "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
        "            final_test_t_list.append(X_deep_temp)\n",
        "        X_deep_t_test = np.concatenate(final_test_t_list, axis=5)\n",
        "\n",
        "        final_test_r_list = []\n",
        "        for ahead_time in ahead_times:\n",
        "            year_test_list = []\n",
        "            for pressure in pressures:\n",
        "                scaler_name = 'r' +str(pressure) + str(ahead_time)\n",
        "                X_deep = reanalysis_r_test_dict[scaler_name][reanalysis_r_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
        "                X_deep = X_deep_r_scaler_dict[scaler_name].transform(X_deep)\n",
        "                X_deep_final = X_deep.reshape(-1, 1, 1, 3, 3, 1)\n",
        "                year_test_list.append(X_deep_final)\n",
        "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
        "            final_test_r_list.append(X_deep_temp)\n",
        "        X_deep_r_test = np.concatenate(final_test_r_list, axis=5)\n",
        "\n",
        "    \n",
        "        X_deep_test = np.concatenate((X_deep_u_test, X_deep_v_test,X_deep_t_test,X_deep_r_test), axis=1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
        "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
        "\n",
        "        pred = net(X_wide_test, X_deep_test)\n",
        "\n",
        "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy().reshape(-1,1))\n",
        "        true = y_test.values.reshape(-1, 1)\n",
        "        diff = np.abs(pred - true)\n",
        "\n",
        "        print('avg wind error:', sum(diff)/len(diff))\n",
        "        total=total+sum(diff)/len(diff)\n",
        "print(\"The mean avg is::\",total/len(test_list))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "o3LytQxr6QiE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}